{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "This notebook is the second project of Deep Reinforcement Learnig Nanodgree(@udacity).\n",
    "\n",
    "\n",
    " - Deep Deterministic Policy Gradient; Soft Updates and Replay buffer implemented in the Unity environment.\n",
    " - Please note that the code from the DDPG lecture was used as the source.\n",
    "\n",
    "All you have to do is run the code cells one by one. You'll see an untrained agent become an expert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure you like Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook placed in the same folder as *'recheacher.exe'* runs fine on **windows10**. It can't be guaranteed the nice results on other operating systems.\n",
    "\n",
    "Let's start the Environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the first ***brain*** available, and set it as the default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(num_agents)\n",
    "for i in range(100):\n",
    "    actions = np.random.randn(num_agents, action_size)\n",
    "    actions = np.clip(actions, -1, 1)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += env_info.rewards\n",
    "    states = next_states\n",
    "    if np.any(dones):\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Actor Critic Network\n",
    "There are two classes in following code cell\n",
    "\n",
    " - Actor: states -> actions\n",
    " - Critic : state and action -> Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):   \n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=320, fc2_units=160):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # Batch Nomalization\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.bn3 = nn.BatchNorm1d(action_size)\n",
    "        # reset paramters\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):        \n",
    "        x = F.relu(self.bn1(self.fc1(state)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        return F.tanh(self.bn3(self.fc3(x)))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=320, fc2_units=160):  \n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)        \n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        # reset paramters\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):        \n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, seed):        \n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):        \n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Noisy process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):    \n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-4\n",
    "WEIGHT_DECAY = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DDPG_agent():\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random_seed\n",
    "                \n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        self.critic_local = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        self.noise = OUNoise((num_agents,action_size), random_seed)\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, random_seed)       \n",
    "        \n",
    "    def act(self, state, add_noise=True):\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        \n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()       \n",
    "        \n",
    "        return np.clip(action, -1, 1)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        for i in range(state.shape[0]):\n",
    "            self.memory.add(state[i, :], action[i], reward[i], next_state[i, :], done[i])\n",
    "                \n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = DDPG_agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)\n",
    "\n",
    "def DDPG(n_episodes=500, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    print_every = 10\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agents.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "       \n",
    "        for t in range(max_t):\n",
    "            action = agents.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            agents.step(state, action, rewards, next_state, dones)\n",
    "            state = next_state\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                print('\\tSteps: ', t)\n",
    "                break \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.3f}'.format(i_episode, \n",
    "                                                                          np.mean(scores_deque), \n",
    "                                                                          np.mean(score)))\n",
    "        average_score = np.mean(scores_deque)\n",
    "        if i_episode % print_every == 20 or average_score > 30:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth') \n",
    "            if average_score > 30:\n",
    "                break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\envs\\drlnd\\lib\\site-packages\\torch\\nn\\functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.48\tScore: 0.482\n",
      "Episode 2\tAverage Score: 0.58\tScore: 0.676\n",
      "Episode 3\tAverage Score: 0.71\tScore: 0.967\n",
      "Episode 4\tAverage Score: 0.76\tScore: 0.921\n",
      "Episode 5\tAverage Score: 0.77\tScore: 0.822\n",
      "Episode 6\tAverage Score: 0.81\tScore: 0.995\n",
      "Episode 7\tAverage Score: 0.90\tScore: 1.417\n",
      "Episode 8\tAverage Score: 1.02\tScore: 1.899\n",
      "Episode 9\tAverage Score: 1.18\tScore: 2.473\n",
      "Episode 10\tAverage Score: 1.34\tScore: 2.729\n",
      "Episode 11\tAverage Score: 1.53\tScore: 3.455\n",
      "Episode 12\tAverage Score: 1.77\tScore: 4.423\n",
      "Episode 13\tAverage Score: 2.05\tScore: 5.398\n",
      "Episode 14\tAverage Score: 2.30\tScore: 5.542\n",
      "Episode 15\tAverage Score: 2.66\tScore: 7.745\n",
      "Episode 16\tAverage Score: 3.07\tScore: 9.139\n",
      "Episode 17\tAverage Score: 3.48\tScore: 10.079\n",
      "Episode 18\tAverage Score: 4.10\tScore: 14.545\n",
      "Episode 19\tAverage Score: 4.67\tScore: 14.925\n",
      "Episode 20\tAverage Score: 5.47\tScore: 20.694\n",
      "Episode 21\tAverage Score: 6.20\tScore: 20.784\n",
      "Episode 22\tAverage Score: 7.10\tScore: 26.124\n",
      "Episode 23\tAverage Score: 8.15\tScore: 31.274\n",
      "Episode 24\tAverage Score: 9.23\tScore: 34.101\n",
      "Episode 25\tAverage Score: 10.31\tScore: 36.188\n",
      "Episode 26\tAverage Score: 11.34\tScore: 36.967\n",
      "Episode 27\tAverage Score: 12.30\tScore: 37.457\n",
      "Episode 28\tAverage Score: 13.21\tScore: 37.568\n",
      "Episode 29\tAverage Score: 14.06\tScore: 37.982\n",
      "Episode 30\tAverage Score: 14.86\tScore: 38.072\n",
      "Episode 31\tAverage Score: 15.61\tScore: 37.961\n",
      "Episode 32\tAverage Score: 16.31\tScore: 38.207\n",
      "Episode 33\tAverage Score: 16.98\tScore: 38.169\n",
      "Episode 34\tAverage Score: 17.58\tScore: 37.503\n",
      "Episode 35\tAverage Score: 18.16\tScore: 37.973\n",
      "Episode 36\tAverage Score: 18.71\tScore: 38.023\n",
      "Episode 37\tAverage Score: 19.23\tScore: 37.727\n",
      "Episode 38\tAverage Score: 19.72\tScore: 37.819\n",
      "Episode 39\tAverage Score: 20.18\tScore: 37.879\n",
      "Episode 40\tAverage Score: 20.62\tScore: 37.801\n",
      "Episode 41\tAverage Score: 21.05\tScore: 38.293\n",
      "Episode 42\tAverage Score: 21.45\tScore: 37.846\n",
      "Episode 43\tAverage Score: 21.83\tScore: 37.569\n",
      "Episode 44\tAverage Score: 22.17\tScore: 36.992\n",
      "Episode 45\tAverage Score: 22.52\tScore: 37.801\n",
      "Episode 46\tAverage Score: 22.84\tScore: 37.409\n",
      "Episode 47\tAverage Score: 23.15\tScore: 37.421\n",
      "Episode 48\tAverage Score: 23.46\tScore: 37.839\n",
      "Episode 49\tAverage Score: 23.75\tScore: 37.893\n",
      "Episode 50\tAverage Score: 24.04\tScore: 37.901\n",
      "Episode 51\tAverage Score: 24.30\tScore: 37.537\n",
      "Episode 52\tAverage Score: 24.56\tScore: 37.657\n",
      "Episode 53\tAverage Score: 24.81\tScore: 38.105\n",
      "Episode 54\tAverage Score: 25.05\tScore: 37.563\n",
      "Episode 55\tAverage Score: 25.29\tScore: 37.960\n",
      "Episode 56\tAverage Score: 25.51\tScore: 37.791\n",
      "Episode 57\tAverage Score: 25.72\tScore: 37.463\n",
      "Episode 58\tAverage Score: 25.92\tScore: 37.436\n",
      "Episode 59\tAverage Score: 26.12\tScore: 37.468\n",
      "Episode 60\tAverage Score: 26.30\tScore: 37.220\n",
      "Episode 61\tAverage Score: 26.49\tScore: 37.820\n",
      "Episode 62\tAverage Score: 26.68\tScore: 37.964\n",
      "Episode 63\tAverage Score: 26.86\tScore: 38.209\n",
      "Episode 64\tAverage Score: 27.03\tScore: 37.641\n",
      "Episode 65\tAverage Score: 27.19\tScore: 37.359\n",
      "Episode 66\tAverage Score: 27.33\tScore: 36.564\n",
      "Episode 67\tAverage Score: 27.47\tScore: 37.087\n",
      "Episode 68\tAverage Score: 27.61\tScore: 36.657\n",
      "Episode 69\tAverage Score: 27.75\tScore: 37.196\n",
      "Episode 70\tAverage Score: 27.89\tScore: 37.603\n",
      "Episode 71\tAverage Score: 28.02\tScore: 37.551\n",
      "Episode 72\tAverage Score: 28.15\tScore: 37.320\n",
      "Episode 73\tAverage Score: 28.28\tScore: 37.498\n",
      "Episode 74\tAverage Score: 28.41\tScore: 37.636\n",
      "Episode 75\tAverage Score: 28.52\tScore: 36.988\n",
      "Episode 76\tAverage Score: 28.64\tScore: 37.548\n",
      "Episode 77\tAverage Score: 28.75\tScore: 37.154\n",
      "Episode 78\tAverage Score: 28.86\tScore: 37.242\n",
      "Episode 79\tAverage Score: 28.97\tScore: 37.380\n",
      "Episode 80\tAverage Score: 29.07\tScore: 36.707\n",
      "Episode 81\tAverage Score: 29.17\tScore: 37.580\n",
      "Episode 82\tAverage Score: 29.27\tScore: 37.384\n",
      "Episode 83\tAverage Score: 29.37\tScore: 37.534\n",
      "Episode 84\tAverage Score: 29.46\tScore: 37.005\n",
      "Episode 85\tAverage Score: 29.55\tScore: 36.765\n",
      "Episode 86\tAverage Score: 29.64\tScore: 37.341\n",
      "Episode 87\tAverage Score: 29.72\tScore: 37.180\n",
      "Episode 88\tAverage Score: 29.81\tScore: 37.123\n",
      "Episode 89\tAverage Score: 29.89\tScore: 37.221\n",
      "Episode 90\tAverage Score: 29.97\tScore: 37.155\n",
      "Episode 91\tAverage Score: 30.05\tScore: 36.733\n",
      "Episode 91\tAverage Score: 30.05\n"
     ]
    }
   ],
   "source": [
    "score = DDPG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy the beautiful rising curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(score)+1), score)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Let's look at the smart agent we trained\n",
    "It is stored in *Checkpoint_xx.pth* trained weights. Let's load this file now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location='cpu'))\n",
    "agents.critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location='cpu'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]        \n",
    "states = env_info.vector_observations                  \n",
    "scores = np.zeros(num_agents)                          \n",
    "\n",
    "for i in range(200):\n",
    "    actions = agents.act(states, add_noise=False)                    \n",
    "    env_info = env.step(actions)[brain_name]        \n",
    "    next_states = env_info.vector_observations        \n",
    "    rewards = env_info.rewards                        \n",
    "    dones = env_info.local_done                 \n",
    "    scores += rewards                         \n",
    "    states = next_states                              \n",
    "    if np.any(dones):                              \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Result\n",
    "\n",
    "- Targeted Average Scores reached  within 91 episodes.\n",
    "- Batch Nomalization help to speed up a bit\n",
    "\n",
    "Thank you"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
